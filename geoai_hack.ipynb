{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ade338c",
   "metadata": {},
   "source": [
    "# GeoAI Hack - Locust Breeding Ground Prediction using HLS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574f053-60d4-419b-88cc-c58d08a8177c",
   "metadata": {
    "id": "4574f053-60d4-419b-88cc-c58d08a8177c"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/instadeepai/InstaGeo-E2E-Geospatial-ML/blob/main/notebooks/InstaGeo_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This starter notebook showcases the capabilities of InstaGeo, an end-to-end package designed for geospatial machine learning with multispectral data.\n",
    "\n",
    "In this demonstration, we use ground-truth locust observations downloaded from the [UN FAO Locust Hub ](https://locust-hub-hqfao.hub.arcgis.com/) on March 17, 2022 to learn a model for identifying desert locust breeding grounds in Africa. The notebook will guide you through the process of creating segmentation-like data from these observations, fine-tuning the [Prithvi](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M) model, and finally visualizing the inference results on an interactive map.\n",
    "\n",
    "By the end of this demo, you will gain hands-on experience with key InstaGeo functionalities and learn how it streamlines geospatial ML workflows from data preparation to model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ead2a",
   "metadata": {
    "id": "380ead2a"
   },
   "source": [
    "# Install InstaGeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550a300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e550a300",
    "outputId": "01be6d02-d659-421f-b382-98e093bfd8b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repository_url = \"https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\"\n",
    "\n",
    "!git clone {repository_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e762b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9f7e762b",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "83e1756d-4ee7-4eb2-ded4-f613973ec8c6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd InstaGeo-E2E-Geospatial-ML\n",
    "pip install -e .[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c78e7-720e-49ed-b5ce-6be6567d2585",
   "metadata": {
    "id": "238c78e7-720e-49ed-b5ce-6be6567d2585"
   },
   "source": [
    "## EarthData Login\n",
    "\n",
    "InstaGeo currently supports multispectral data from NASA [Harmonized Landsat and Sentinel-2 (HLS)](https://hls.gsfc.nasa.gov/). Accessing HLS data requires an EarthData user account which can be created [here](https://urs.earthdata.nasa.gov/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7ff9c",
   "metadata": {
    "id": "4fc7ff9c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a5f61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b7a5f61",
    "outputId": "74e752c9-e666-4363-b02f-66a0780fc240",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter you EarthData user account credentials\n",
    "USERNAME = getpass('Enter your EarthData username: ')\n",
    "PASSWORD = getpass('Enter your EarthData password: ')\n",
    "\n",
    "content = f\"\"\"machine urs.earthdata.nasa.gov login {USERNAME} password {PASSWORD}\"\"\"\n",
    "\n",
    "with open(os.path.expanduser('~/.netrc'), 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a15d3-f22e-4b2c-85c0-435c832e708c",
   "metadata": {
    "id": "488a15d3-f22e-4b2c-85c0-435c832e708c"
   },
   "source": [
    "## InstaGeo - Data (Optional)\n",
    "\n",
    "With InstaGeo installed and EarthData authentication configured, we are now ready to download and process HLS (Harmonized Landsat and Sentinel) granules using the `InstaGeo-Data` module. This module offers several powerful functionalities for handling geospatial data, including:\n",
    "\n",
    "- Searching and retrieving metadata for HLS granules\n",
    "- Downloading specific spectral bands from HLS granules\n",
    "- Generating data chips and corresponding target labels for machine learning tasks\n",
    "\n",
    "These capabilities streamline the preprocessing of multispectral data, setting the foundation for efficient geospatial model development.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e36f50",
   "metadata": {
    "id": "38e36f50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d83f72",
   "metadata": {
    "id": "c4d83f72"
   },
   "source": [
    "The ground-truth locust observations used in this challenge were downloaded from the [UN FAO Locust Hub ](https://locust-hub-hqfao.hub.arcgis.com/) on March 17, 2022. The raw data was processed to derive our locust breeding ground dataset. The label dataset was splitted into train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc545f4",
   "metadata": {
    "id": "fcc545f4"
   },
   "source": [
    "After splitting the data into training and test splits, the next step is to group the data by the HLS granules they belong to and download the corresponding spectral bands for each granule. Once the bands are retrieved, we will generate smaller chips and target labels with dimensions of 256 x 256 pixels.\n",
    "\n",
    "By the end of this process, the input data will have a shape of 3 x 6 x 256 x 256 (representing three sets of six spectral bands and 256 x 256 pixel chips), and the target labels will have a shape of 256 x 256.\n",
    "\n",
    "While these tasks might seem complex, the `InstaGeo-Data` module abstracts this process, allowing you to configure it with a simple command as shown in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_mapping(root_dir, input_subdir, output_csv):\n",
    "    \"\"\"\n",
    "    Generate a CSV mapping input chips to corresponding segmentation maps.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n",
    "        input_subdir (str): Subdirectory path for chips within the root directory.\n",
    "        output_csv (str or Path): Output path for the generated CSV file.\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n",
    "\n",
    "    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n",
    "    seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n",
    "\n",
    "    df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n",
    "    df.to_csv(root_dir / output_csv, index=False)\n",
    "    \n",
    "    print(f\"Number of rows is: {df.shape[0]}\")\n",
    "    print(f\"CSV generated and saved to: {root_dir / output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901a48e",
   "metadata": {
    "id": "b901a48e"
   },
   "source": [
    "### Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bfeb8-df6b-4b8c-8c8a-059a5a28b8ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "c11bfeb8-df6b-4b8c-8c8a-059a5a28b8ea",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2b68bdfe-c648-44ed-c121-02177df206fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir train\n",
    "\n",
    "!python -m \"instageo.data.chip_creator\" \\\n",
    "    --dataframe_path=\"train.csv\" \\\n",
    "    --output_directory=\"train\" \\\n",
    "    --min_count=10 \\\n",
    "    --chip_size=256 \\\n",
    "    --temporal_tolerance=3 \\\n",
    "    --temporal_step=30 \\\n",
    "    --num_steps=3 \\\n",
    "    --masking_strategy=any \\\n",
    "    --mask_types=water \\\n",
    "    --window_size=3 \\\n",
    "    --processing_method=cog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22FADUFcfYsR",
   "metadata": {
    "id": "22FADUFcfYsR"
   },
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), \"train\", \"train_ds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e7fa6",
   "metadata": {
    "id": "3d4e7fa6"
   },
   "source": [
    "### Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_xaa5V3sf3pJ",
   "metadata": {
    "id": "_xaa5V3sf3pJ"
   },
   "outputs": [],
   "source": [
    "!mkdir test\n",
    "\n",
    "!python -m \"instageo.data.chip_creator\" \\\n",
    "    --dataframe_path=\"test.csv\" \\\n",
    "    --output_directory=\"test\" \\\n",
    "    --min_count=1 \\\n",
    "    --chip_size=256 \\\n",
    "    --temporal_tolerance=3 \\\n",
    "    --temporal_step=30 \\\n",
    "    --num_steps=3 \\\n",
    "    --masking_strategy=any \\\n",
    "    --mask_types=water \\\n",
    "    --processing_method=cog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XYrUAef1mJ0N",
   "metadata": {
    "id": "XYrUAef1mJ0N"
   },
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), \"test\", \"test_ds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ef759",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Due to the limited time available for this hackathon, we have created the chips and labels using `Instageo-Data`, which took 57h to complete.\n",
    "\n",
    "The data is provided as part of this competition. So you can simply download it and start hacking your way to a TOP SOLUTION!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4ba26",
   "metadata": {},
   "source": [
    "Extract the compressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf train.tar.gz\n",
    "!tar -xvzf test.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd9ebec",
   "metadata": {},
   "source": [
    "Create input and label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_mapping(root_dir, input_subdir, output_csv):\n",
    "    \"\"\"\n",
    "    Generate a CSV mapping input chips to corresponding segmentation maps.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n",
    "        input_subdir (str): Subdirectory path for chips within the root directory.\n",
    "        output_csv (str or Path): Output path for the generated CSV file.\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n",
    "\n",
    "    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n",
    "    seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n",
    "\n",
    "    df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n",
    "    df.to_csv(root_dir / output_csv, index=False)\n",
    "    \n",
    "    print(f\"Number of rows is: {df.shape[0]}\")\n",
    "    print(f\"CSV generated and saved to: {root_dir / output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d992337",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), 'train', \"train_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), 'test', \"test_ds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3955a5",
   "metadata": {},
   "source": [
    "Split out Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_validation_data(mapping_csv, data_dir, train_dir, validation_dir, validation_split=0.3):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets based on a CSV file mapping `chips` and `seg_maps`.\n",
    "\n",
    "    Args:\n",
    "        mapping_csv (str or Path): Path to the CSV file containing the mapping between `chips` and `seg_maps`.\n",
    "        data_dir (str or Path): Path to the merged directory containing all files.\n",
    "        validation_dir (str or Path): Path to the new directory for validation files.\n",
    "        validation_split (float): Fraction of the data to use as the validation set.\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    validation_dir = Path(validation_dir)\n",
    "    train_dir = Path(train_dir)\n",
    "\n",
    "    validation_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(mapping_csv)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    num_val = int(len(df) * validation_split)\n",
    "    train_df = df[num_val:]\n",
    "    val_df = df[:num_val]\n",
    "\n",
    "    for _, row in val_df.iterrows():\n",
    "        chip_file = data_dir / Path(row['Input']).relative_to(data_dir)\n",
    "        seg_map_file = data_dir / Path(row['Label']).relative_to(data_dir)\n",
    "\n",
    "        for file, subfolder in [(chip_file, \"chips\"), (seg_map_file, \"seg_maps\")]:\n",
    "            if file.exists():\n",
    "                dest_path = validation_dir / subfolder / file.relative_to(data_dir).name\n",
    "                dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.move(str(file), str(dest_path))\n",
    "            else:\n",
    "                print(f\"File not found: {file}\")\n",
    "                \n",
    "    for _, row in train_df.iterrows():\n",
    "        chip_file = data_dir / Path(row['Input']).relative_to(data_dir)\n",
    "        seg_map_file = data_dir / Path(row['Label']).relative_to(data_dir)\n",
    "\n",
    "        for file, subfolder in [(chip_file, \"chips\"), (seg_map_file, \"seg_maps\")]:\n",
    "            if file.exists():\n",
    "                dest_path = train_dir / subfolder / file.relative_to(data_dir).name\n",
    "                dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.move(str(file), str(dest_path))\n",
    "            else:\n",
    "                print(f\"File not found: {file}\")\n",
    "                \n",
    "    print(f\"Train files moved to {train_dir}. Train set size: {len(train_df)}.\")\n",
    "    print(f\"Validation files moved to {validation_dir}. Validation set size: {len(val_df)}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1303180",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_validation_data(\n",
    "    mapping_csv=\"train_ds.csv\",\n",
    "    data_dir=\"train_split\",\n",
    "    validation_dir=\"validation_split\",\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), 'train_split', \"train_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c014320",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_label_mapping(Path.cwd(), 'validation_split', \"validation_split.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb132b0-04dd-4583-8e30-e6332446a0e6",
   "metadata": {
    "id": "bcb132b0-04dd-4583-8e30-e6332446a0e6"
   },
   "source": [
    "## InstaGeo - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd313044-829c-482d-934e-9ae662f132fc",
   "metadata": {
    "id": "fd313044-829c-482d-934e-9ae662f132fc"
   },
   "source": [
    "After creating our dataset using the `InstaGeo-Data` module, we can move on to fine-tuning a model that includes a Prithvi backbone paired with a classification head. For regression tasks, the classification head can easily be replaced with a suitable regression head. Additionally, if a completely different model architecture is needed, it can be designed and implemented within this framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fee29-dca3-4611-a43a-4412a6033751",
   "metadata": {
    "id": "c64fee29-dca3-4611-a43a-4412a6033751"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cbc92",
   "metadata": {
    "id": "115cbc92"
   },
   "source": [
    "**Launch Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3672a",
   "metadata": {},
   "source": [
    "First compute the mean and standard deviation for the dataset and update the corresponding config file, in this case `locust.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4a042-ab38-470f-ad02-3dd67ddda0c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir='.' \\\n",
    "    train.batch_size=8 \\\n",
    "    train.num_epochs=5 \\\n",
    "    mode=stats \\\n",
    "    train_filepath=\"train_ds.csv\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264ca12",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir='.' \\\n",
    "    train.batch_size=8 \\\n",
    "    train.num_epochs=5 \\\n",
    "    mode=train \\\n",
    "    train_filepath=\"train_ds.csv\" \\\n",
    "    valid_filepath=\"val_ds.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db019e",
   "metadata": {
    "id": "10db019e"
   },
   "source": [
    "**Run Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acf232",
   "metadata": {
    "id": "67acf232"
   },
   "source": [
    "Adjust the `checkpoint_path` argument to use the desired model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8230588-2842-460c-b2e7-4f2f4175dc3e",
   "metadata": {
    "collapsed": true,
    "id": "f8230588-2842-460c-b2e7-4f2f4175dc3e",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir='.' \\\n",
    "    test_filepath=\"test_ds.csv\" \\\n",
    "    train.batch_size=8 \\\n",
    "    checkpoint_path='checkpoint-path' \\\n",
    "    mode=eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3646bd",
   "metadata": {},
   "source": [
    "### Make Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070575a",
   "metadata": {},
   "source": [
    "We first run inference on test chips to get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir='.' \\\n",
    "    test_filepath=\"test_ds.csv\" \\\n",
    "    train.batch_size=16 \\\n",
    "    checkpoint_path='checkpoint_path' \\\n",
    "    mode=chip_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642710d0",
   "metadata": {},
   "source": [
    "After getting the prdictions for each chip, we retrieve the predicted value for each observatio in our test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165da191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import CRS, Transformer\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "predictions_directory = \"predictions\"\n",
    "prediction_files = os.listdir(predictions_directory)\n",
    "\n",
    "def get_prediction_value(row):\n",
    "    matching_files = [f for f in prediction_files if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n",
    "    if not matching_files:\n",
    "        return (np.nan, np.nan)\n",
    "    for file in matching_files:\n",
    "        with rasterio.open(f\"{predictions_directory}/{file}\") as src:\n",
    "            width, height = src.width, src.height\n",
    "            affine_transform = rasterio.transform.AffineTransformer(src.transform)\n",
    "            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n",
    "            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n",
    "            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n",
    "            \n",
    "            if 0 <= x_offset < width and 0 <= y_offset < height:\n",
    "                return src.read(1)[x_offset, y_offset], file\n",
    "    return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(\"hls_submission.csv\")\n",
    "submission_df[['prediction', 'filename']] = submission_df.apply(get_prediction_value, axis=1, result_type='expand')\n",
    "submission_df.to_csv(\"hls_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fb43d",
   "metadata": {},
   "source": [
    "**Upload submission file to Kaggle to see leaderboard score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a73d79-f69f-4fb8-aa63-768ee3ee47ea",
   "metadata": {
    "id": "b9a73d79-f69f-4fb8-aa63-768ee3ee47ea"
   },
   "source": [
    "**Run Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YgxyC5GnvjJU",
   "metadata": {
    "collapsed": true,
    "id": "YgxyC5GnvjJU",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !gsutil cp gs://instageo/utils/africa_prediction_template.csv .\n",
    "!mkdir -p inference/2021-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30afd0-73ec-4eb1-b9c1-a2d36475eae7",
   "metadata": {
    "id": "9c30afd0-73ec-4eb1-b9c1-a2d36475eae7"
   },
   "source": [
    "**Create Inference Data**\n",
    "\n",
    "For inference, we only need to download the necessary HLS tiles and run inference directly using the sliding window inference feature.\n",
    "\n",
    "If you're running inference across the entire African continent, you can use the `africa_prediction_template.csv`, which will automatically download 2,120 HLS granules covering Africa and parts of Asia.\n",
    "\n",
    "For this demo, we'll limit the scope to the HLS granules included in our test split.\n",
    "\n",
    "Note: Ensure you have approximately 1TB of storage space available for this process if you are running inference across Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b3ea-8ef7-473b-8f2e-72d493fdba03",
   "metadata": {
    "collapsed": true,
    "id": "8f76b3ea-8ef7-473b-8f2e-72d493fdba03",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m \"instageo.data.chip_creator\" \\\n",
    "#     --dataframe_path=\"africa_prediction_template.csv\" \\\n",
    "#     --output_directory=\"inference/2021-06\" \\\n",
    "#     --min_count=1 \\\n",
    "#     --no_data_value=-1 \\\n",
    "#     --temporal_tolerance=3 \\\n",
    "#     --temporal_step=30 \\\n",
    "#     --num_steps=3 \\\n",
    "#     --download_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ku-49mTIqWvW",
   "metadata": {
    "id": "ku-49mTIqWvW"
   },
   "outputs": [],
   "source": [
    "# Instead of downloading new set of HLS tiles, we can use the one for our test split for inference.\n",
    "\n",
    "!cp -r test/* inference/2021-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e86341-0717-458f-b061-3e957ce8536d",
   "metadata": {
    "id": "65e86341-0717-458f-b061-3e957ce8536d"
   },
   "source": [
    "**Run Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a510ba",
   "metadata": {
    "id": "80a510ba"
   },
   "source": [
    "Adjust the `checkpoint_path` argument to use the desired model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JJXq8oWNAr1w",
   "metadata": {
    "id": "JJXq8oWNAr1w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir='inference/2021-06' \\\n",
    "    test_filepath='hls_dataset.json' \\\n",
    "    train.batch_size=16 \\\n",
    "    test.mask_cloud=True \\\n",
    "    checkpoint_path='checkpoint-path' \\\n",
    "    mode=predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79e412-8176-411a-8a73-6068e22a0d25",
   "metadata": {
    "id": "8d79e412-8176-411a-8a73-6068e22a0d25"
   },
   "source": [
    "## InstaGeo - Apps\n",
    "Once inference has been completed on the HLS tiles and the results have been saved, we can use the `InstaGeo-Apps` module to visualize the predictions on an interactive map.\n",
    "\n",
    "To visualize the results, simply move the HLS prediction GeoTIFF files to the appropriate directory, and `InstaGeo-Apps` will handle the rest, providing an intuitive and interactive mapping experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a904882-cde9-40c8-affb-f988892e5183",
   "metadata": {
    "id": "8a904882-cde9-40c8-affb-f988892e5183",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p predictions/2023/6\n",
    "!mv inference/2023-06/predictions/* /content/predictions/2023/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tQGnk67MY6Cd",
   "metadata": {
    "id": "tQGnk67MY6Cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BeJsQzkeBNm7",
   "metadata": {
    "collapsed": true,
    "id": "BeJsQzkeBNm7",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nohup streamlit run InstaGeo-E2E-Geospatial-ML/instageo/apps/app.py --server.address=localhost &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48459844-9277-473b-8cd8-e47691c59c0a",
   "metadata": {
    "id": "48459844-9277-473b-8cd8-e47691c59c0a"
   },
   "source": [
    "Retrieve your IP address which is the password of the localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S5rCS-lVZiWe",
   "metadata": {
    "id": "S5rCS-lVZiWe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "print(\"Password/Endpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bB61RRBNY-48",
   "metadata": {
    "id": "bB61RRBNY-48",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d922e",
   "metadata": {
    "id": "9f7d922e"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated the end-to-end capabilities of InstaGeo for geospatial machine learning using multispectral data. We began by downloading and processing HLS granules, creating data chips for training, and fine-tuning a model with the Prithvi backbone. Finally, we ran inference on test data and visualized the results using the `InstaGeo-Apps` module.\n",
    "\n",
    "By leveraging InstaGeo, complex tasks such as data preprocessing, model training, and large-scale inference can be streamlined and efficiently handled with minimal configuration.\n",
    "\n",
    "If you found this demo helpful, please consider giving our [InstaGeo GitHub repository](https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML) a star â­! Your support helps us continue improving the tool for the community.\n",
    "\n",
    "Thank you for exploring InstaGeo with us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316908ce",
   "metadata": {
    "id": "316908ce"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": ".m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
